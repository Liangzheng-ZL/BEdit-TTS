# network architecture related
model-module: espnet.nets.pytorch_backend.e2e_tts_bedit:FeedForwardTransformer
adim: 512
aheads: 2
elayers: 6
eunits: 1536
dlayers: 6
dunits: 1536
duration-predictor-layers: 2
duration-predictor-chans: 256
duration-predictor-kernel-size: 3
positionwise-layer-type: conv1d
positionwise-conv-kernel-size: 3
postnet-layers: 5
postnet-filts: 5
postnet-chans: 256
use-batch-norm: True
use-scaled-pos-enc: True
encoder-normalize-before: False
decoder-normalize-before: False
encoder-concat-after: False
decoder-concat-after: False
reduction-factor: 1
use-speaker-embedding: false


# global condition
ref-global-enc-filters: [32, 32, 64, 64, 128, 128]
ref-global-enc-gru-size: 64
global-condition-num-gaussian: 5
sample-global-condition-top-k-components: 1
# phone-level condition
ref-pl-enc-filters: [ 8, 8 ]
ref-pl-enc-gru-size: 64
pl-condition-num-gaussian: 20
sample-pl-condition-top-k-components: 3
# speaker embedding
spk-embed-dim: 128
num-speakers: 10
num-extra-speakers: 20
##########################################################

# minibatch related
# NOTE: the batch bins are carefully selected so that the average batch size will
# be 32 and it asummes the number of vocabularies is 77 (in phoneme case)
# If you want to use char input instead, you will need to tweak the batch-bin.
batch-sort-key: output # shuffle or input or output
batch-bins: 1600000
          # 1624061   # 32 * (566.506 * 80 + 70.538 * 77)
                      # avg batch-size * (mean_out * dim_out + mean_in * dim_in)
                      # 402 batches containing from 20 to 121 samples (avg 31 samples)

# training related
transformer-init: pytorch
transformer-warmup-steps: 4000
transformer-lr: 1.0
initial-encoder-alpha: 1.0
initial-decoder-alpha: 1.0
transformer-enc-dropout-rate: 0.1
transformer-enc-positional-dropout-rate: 0.1
transformer-enc-attn-dropout-rate: 0.1
transformer-dec-dropout-rate: 0.1
transformer-dec-positional-dropout-rate: 0.1
transformer-dec-attn-dropout-rate: 0.1
transformer-enc-dec-attn-dropout-rate: 0.1
postnet-dropout-rate: 0.5
duration-predictor-dropout-rate: 0.1

# optimization related
opt: noam
accum-grad: 2
grad-clip: 1.0
weight-decay: 0.0
patience: 0
epochs: 150  # 700 epochs * 402 batches / 2 accum-grad = 140,700 iters
# teacher-model: exp/phn_train_no_dev_pytorch_train_pytorch_transformer.v3/results/model.last1.avg.best

# other
save-interval-epochs: 1
eval-interval-epochs: 10
num-iter-processes: 4
